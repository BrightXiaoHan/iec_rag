#!/usr/bin/env python3
"""
Xinference è®¾ç½®è„šæœ¬

è¿™ä¸ªè„šæœ¬å¸®åŠ©è‡ªåŠ¨è®¾ç½®å’Œå¯åŠ¨ Xinference æ‰€éœ€çš„æ¨¡å‹
"""

import json
import os
import sys
import time
from typing import Any, Dict

import requests

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(
    0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "src")
)

# Xinference é…ç½®
XINFERENCE_HOST = os.getenv("XINFERENCE_HOST", "localhost")
XINFERENCE_PORT = os.getenv("XINFERENCE_PORT", "9997")
XINFERENCE_BASE_URL = f"http://{XINFERENCE_HOST}:{XINFERENCE_PORT}"

# æ¨¡å‹é…ç½®
MODELS_TO_LAUNCH = [
    {
        "model_name": "bge-base-en-v1.5",
        "model_type": "embedding",
        "description": "BGE åµŒå…¥æ¨¡å‹",
    },
    {
        "model_name": "bge-reranker-base",
        "model_type": "rerank",
        "description": "BGE é‡æ’åºæ¨¡å‹",
    },
]


def check_xinference_server() -> bool:
    """æ£€æŸ¥ Xinference æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get(f"{XINFERENCE_BASE_URL}/v1/models", timeout=5)
        return response.status_code == 200
    except Exception:
        return False


def list_running_models() -> Dict[str, Any]:
    """åˆ—å‡ºæ­£åœ¨è¿è¡Œçš„æ¨¡å‹"""
    try:
        response = requests.get(f"{XINFERENCE_BASE_URL}/v1/models")
        if response.status_code == 200:
            return response.json()
        return {}
    except Exception as e:
        print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return {}


def launch_model(model_name: str, model_type: str) -> bool:
    """å¯åŠ¨æŒ‡å®šçš„æ¨¡å‹"""
    try:
        payload = {"model_name": model_name, "model_type": model_type}

        response = requests.post(
            f"{XINFERENCE_BASE_URL}/v1/models",
            headers={"Content-Type": "application/json"},
            data=json.dumps(payload),
            timeout=60,
        )

        if response.status_code == 200:
            print(f"âœ… æˆåŠŸå¯åŠ¨æ¨¡å‹: {model_name}")
            return True
        else:
            print(f"âŒ å¯åŠ¨æ¨¡å‹å¤±è´¥: {model_name}, çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”: {response.text}")
            return False

    except Exception as e:
        print(f"âŒ å¯åŠ¨æ¨¡å‹å¼‚å¸¸: {model_name}, é”™è¯¯: {e}")
        return False


def is_model_running(model_name: str, running_models: Dict) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»åœ¨è¿è¡Œ"""
    if not running_models:
        return False

    for model_id, model_info in running_models.items():
        if model_info.get("model_name") == model_name:
            return True
    return False


def setup_models():
    """è®¾ç½®æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®¾ç½® Xinference æ¨¡å‹...")

    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    if not check_xinference_server():
        print("âŒ Xinference æœåŠ¡å™¨æœªè¿è¡Œ!")
        print("è¯·å…ˆå¯åŠ¨ Xinference æœåŠ¡å™¨:")
        print(f"xinference-local --host {XINFERENCE_HOST} --port {XINFERENCE_PORT}")
        return False

    print("âœ… Xinference æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")

    # è·å–å½“å‰è¿è¡Œçš„æ¨¡å‹
    running_models = list_running_models()
    print(f"å½“å‰è¿è¡Œçš„æ¨¡å‹æ•°é‡: {len(running_models)}")

    # å¯åŠ¨éœ€è¦çš„æ¨¡å‹
    success_count = 0
    for model_config in MODELS_TO_LAUNCH:
        model_name = model_config["model_name"]
        model_type = model_config["model_type"]
        description = model_config["description"]

        print(f"\nğŸ“¦ å¤„ç†æ¨¡å‹: {description} ({model_name})")

        if is_model_running(model_name, running_models):
            print(f"âœ… æ¨¡å‹å·²åœ¨è¿è¡Œ: {model_name}")
            success_count += 1
        else:
            print(f"ğŸ”„ å¯åŠ¨æ¨¡å‹: {model_name}")
            if launch_model(model_name, model_type):
                success_count += 1
                # ç­‰å¾…æ¨¡å‹å¯åŠ¨
                time.sleep(2)

    print(f"\nğŸ“Š è®¾ç½®å®Œæˆ: {success_count}/{len(MODELS_TO_LAUNCH)} ä¸ªæ¨¡å‹æˆåŠŸ")

    if success_count == len(MODELS_TO_LAUNCH):
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹è®¾ç½®æˆåŠŸ!")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æ¨¡å‹è®¾ç½®å¤±è´¥")
        return False


def show_model_status():
    """æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€"""
    print("\nğŸ“‹ å½“å‰æ¨¡å‹çŠ¶æ€:")

    running_models = list_running_models()

    if not running_models:
        print("æ²¡æœ‰è¿è¡Œä¸­çš„æ¨¡å‹")
        return

    for model_id, model_info in running_models.items():
        model_name = model_info.get("model_name", "æœªçŸ¥")
        model_type = model_info.get("model_type", "æœªçŸ¥")
        print(f"  - {model_name} ({model_type}) [ID: {model_id}]")


def test_models():
    """æµ‹è¯•æ¨¡å‹åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠŸèƒ½...")

    try:
        # æµ‹è¯•åµŒå…¥æ¨¡å‹
        from agent.milvus_search import get_xinference_embedding

        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        embedding = get_xinference_embedding(test_text)

        if embedding:
            print(f"âœ… åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
        else:
            print("âŒ åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥")

    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    print("Xinference æ¨¡å‹è®¾ç½®å·¥å…·")
    print("=" * 50)

    try:
        # è®¾ç½®æ¨¡å‹
        if setup_models():
            # æ˜¾ç¤ºçŠ¶æ€
            show_model_status()

            # æµ‹è¯•åŠŸèƒ½
            test_models()

            print("\nâœ… è®¾ç½®å®Œæˆ! ç°åœ¨å¯ä»¥ä½¿ç”¨çŸ¥è¯†åº“æœç´¢åŠŸèƒ½äº†ã€‚")
        else:
            print("\nâŒ è®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ è®¾ç½®è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()